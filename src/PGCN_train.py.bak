"""
Train PGCN inside LibEER pipeline.

This script uses the integrated PGCN model from EEG.models.PGCN which includes
LibEER-compatible initialization and device synchronization.

Usage examples:
  python PGCN_train.py -metrics acc macro-f1 \
    -metric_choose macro-f1 -setting seed_sub_dependent_front_back_setting \
    -dataset seed_de_lds -batch_size 32 -epochs 80 -lr 0.0015 -model PGCN

Note: ensure you run from a context where project root is importable.
This script bootstraps sys.path for both EEG and LibEER packages.
"""

import sys
from pathlib import Path

# ---- sys.path bootstrap: project root and LibEER package ----
_here = Path(__file__).resolve()
_proj_root = _here.parents[2]  # /home/ako/Project/work
_libeer_root = _proj_root / 'lib' / 'libeer-ako'
for p in (str(_proj_root), str(_libeer_root)):
    if p not in sys.path:
        sys.path.insert(0, p)

import torch
import torch.optim as optim
import torch.nn as nn

try:
    import pytorch_warmup as warmup
    WARMUP_AVAILABLE = True
except ImportError:
    print("[WARNING] pytorch_warmup not installed. Install with: pip install pytorch-warmup")
    WARMUP_AVAILABLE = False

# Import PGCN from integrated models module (no longer using separate adapter)
from EEG.models.PGCN import PGCN
from EEG.utils.Models import Model  # Register PGCN in Model dict
Model['PGCN'] = PGCN

from LibEER.config.setting import preset_setting, set_setting_by_args
from LibEER.data_utils.load_data import get_data
from LibEER.data_utils.split import merge_to_part, index_to_data, get_split_index
from LibEER.utils.args import get_args_parser
from LibEER.utils.store import make_output_dir
from LibEER.utils.utils import state_log, result_log, setup_seed, sub_result_log
from LibEER.Trainer.training import train


def main(args):
    # build setting
    if args.setting is not None:
        setting = preset_setting[args.setting](args)
    else:
        setting = set_setting_by_args(args)
    setup_seed(args.seed)

    # load data and split
    data, label, channels, feature_dim, num_classes = get_data(setting)
    data, label = merge_to_part(data, label, setting)
    device = torch.device(args.device)

    best_metrics = []
    subjects_metrics = [[] for _ in range(len(data))] if setting.experiment_mode == 'subject-dependent' else []
    for rridx, (data_i, label_i) in enumerate(zip(data, label), 1):
        tts = get_split_index(data_i, label_i, setting)
        for ridx, (train_indexes, test_indexes, val_indexes) in enumerate(zip(tts['train'], tts['test'], tts['val']), 1):
            setup_seed(args.seed)
            if val_indexes[0] == -1:
                print(f"train indexes:{train_indexes}, test indexes:{test_indexes}")
            else:
                print(f"train indexes:{train_indexes}, val indexes:{val_indexes}, test indexes:{test_indexes}")

            # split and keep_dim as specified in args
            train_data, train_label, val_data, val_label, test_data, test_label = \
                index_to_data(data_i, label_i, train_indexes, test_indexes, val_indexes, args.keep_dim)

            # fallback: if no val provided, use test as val
            if len(val_data) == 0:
                val_data = test_data
                val_label = test_label

            # build PGCN model
            model = Model['PGCN'](channels, feature_dim, num_classes)
            # # datasets with proper dtypes
            # X_train = torch.as_tensor(train_data, dtype=torch.float32)
            # X_val   = torch.as_tensor(val_data,   dtype=torch.float32)
            # X_test  = torch.as_tensor(test_data,  dtype=torch.float32)

            # # Normalize labels: accept either class indices (N,) or one-hot/soft (N, num_classes)
            # def _to_index_labels(y, n_class):
            #     t = torch.as_tensor(y)
            #     if t.ndim >= 2 and t.shape[-1] == n_class:
            #         t = torch.argmax(t, dim=-1)
            #     return t.to(dtype=torch.long).view(-1)

            # y_train = _to_index_labels(train_label, num_classes)
            # y_val   = _to_index_labels(val_label,   num_classes)
            # y_test  = _to_index_labels(test_label,  num_classes)

            # dataset_train = torch.utils.data.TensorDataset(X_train, y_train)
            # dataset_val   = torch.utils.data.TensorDataset(X_val,   y_val)
            # dataset_test  = torch.utils.data.TensorDataset(X_test,  y_test)
            
            dataset_train = torch.utils.data.TensorDataset(torch.Tensor(train_data), torch.Tensor(train_label))
            dataset_val = torch.utils.data.TensorDataset(torch.Tensor(val_data), torch.Tensor(val_label))
            dataset_test = torch.utils.data.TensorDataset(torch.Tensor(test_data), torch.Tensor(test_label))
            
            # PGCN optimizer: use parameter groups with different LRs for adj/local/weight
            # Separate parameters into groups following PGCN reference implementation
            lap_params, local_params, weight_params = [], [], []
            for pname, p in model.named_parameters():
                if 'adj' in str(pname).lower() or 'lap' in str(pname).lower():
                    lap_params.append(p)
                elif 'local' in str(pname).lower():
                    local_params.append(p)
                else:
                    weight_params.append(p)
            
            # Use beta (default 5e-5) for adjacency, lr for others
            beta = float(getattr(args, 'beta', 5e-5))
            weight_decay = float(getattr(args, 'weight_decay', 5e-4))
            optimizer = optim.AdamW([
                {'params': lap_params, 'lr': beta},
                {'params': local_params, 'lr': args.lr},
                {'params': weight_params, 'lr': args.lr},
            ], betas=(0.9, 0.999), weight_decay=weight_decay, eps=1e-4)
            
            # MultiStepLR scheduler: decay at 1/3 of total epochs
            scheduler = optim.lr_scheduler.MultiStepLR(
                optimizer, milestones=[args.epochs // 3], gamma=0.1
            )
            
            # Warmup scheduler (from PGCN paper)
            warmup_scheduler = None
            if WARMUP_AVAILABLE:
                warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)
                warmup_scheduler.last_step = -1
            
            # Loss function with label smoothing (using custom implementation for compatibility)
            smoothing = float(getattr(args, 'label_smoothing', 0.0) or 0.0)
            if smoothing > 0:
                # Import CE_Label_Smooth_Loss from reference implementation
                try:
                    _ref_root = _proj_root / 'Reference' / 'PGCN' / 'PGCN'
                    if str(_ref_root) not in sys.path:
                        sys.path.insert(0, str(_ref_root))
                    from ...Reference.PGCN.PGCN.utils import CE_Label_Smooth_Loss
                    criterion = CE_Label_Smooth_Loss(classes=num_classes, epsilon=smoothing).to(device)
                    print(f"[PGCN] Using CE_Label_Smooth_Loss with epsilon={smoothing}")
                except ImportError:
                    print(f"[WARNING] Could not import CE_Label_Smooth_Loss, using PyTorch's label_smoothing")
                    criterion = nn.CrossEntropyLoss(label_smoothing=smoothing)
            else:
                criterion = nn.CrossEntropyLoss(label_smoothing=smoothing)
            
            # Output directory
            output_dir = make_output_dir(args, 'PGCN')

            # train (pass scheduler and warmup_scheduler for PGCN)
            round_metric = train(
                model=model,
                dataset_train=dataset_train,
                dataset_val=dataset_val,
                dataset_test=dataset_test,
                device=device,
                output_dir=output_dir,
                metrics=args.metrics,
                metric_choose=args.metric_choose,
                optimizer=optimizer,
                scheduler=scheduler,
                warmup_scheduler=warmup_scheduler,
                batch_size=args.batch_size,
                epochs=args.epochs,
                criterion=criterion,
                loss_func=None,
                loss_param=None,
            )
            best_metrics.append(round_metric)
            if setting.experiment_mode == 'subject-dependent':
                subjects_metrics[rridx - 1].append(round_metric)

    if setting.experiment_mode == 'subject-dependent':
        sub_result_log(args, subjects_metrics)
    else:
        result_log(args, best_metrics)


if __name__ == '__main__':
    parser = get_args_parser()
    # Extend base parser with PGCN-specific hyperparameters
    parser.add_argument('-label_smoothing', default=0.0, type=float,
                        help='Label smoothing epsilon for CrossEntropyLoss (0.0 disables)')
    parser.add_argument('-weight_decay', default=5e-4, type=float,
                        help='Weight decay (L2 regularization) for optimizer')
    parser.add_argument('-beta', default=5e-5, type=float,
                        help='Learning rate for adjacency matrix parameters (PGCN-specific)')
    args = parser.parse_args()
    # If user didn't explicitly pass -model/--model, default to PGCN in this script
    if not any(flag in sys.argv for flag in ('-model', '--model')):
        args.model = 'PGCN'
    
    # Device sanity check and informative prints
    print('='*60)
    print('[PCGN_train] Training Configuration')
    print('='*60)
    try:
        import torch as _torch
        cuda_available = _torch.cuda.is_available()
        print(f'CUDA available: {cuda_available}')
        if cuda_available:
            print(f'PyTorch CUDA version: {_torch.version.cuda}')
            print(f'CUDA device name: {_torch.cuda.get_device_name(0)}')
            print(f'CUDA device count: {_torch.cuda.device_count()}')
        
        # Use the device specified by user (default is 'cuda')
        target_device = getattr(args, 'device', 'cuda')
        if target_device == 'cuda' and not cuda_available:
            print('[WARNING] CUDA requested but not available, falling back to CPU')
            args.device = 'cpu'
        else:
            args.device = target_device
        print(f'Training device: {args.device}')
    except Exception as _e:
        print('[PCGN_train] Device check failed:', _e)
        args.device = 'cpu'
    
    print(f'Model: {args.model}')
    print(f'Dataset: {getattr(args, "dataset", "default")}')
    print(f'Batch size: {getattr(args, "batch_size", 32)}')
    print(f'Learning rate: {getattr(args, "lr", 0.001)}')
    print(f'Beta (adj LR): {getattr(args, "beta", 5e-5)}')
    print(f'Weight decay: {getattr(args, "weight_decay", 5e-4)}')
    print(f'Label smoothing: {getattr(args, "label_smoothing", 0.0)}')
    print(f'Epochs: {getattr(args, "epochs", 40)}')
    print('='*60)
    
    # log out train state
    state_log(args)
    main(args)
