"""
é€šç”¨çš„ Subject-wise æ€§èƒ½å¯¹æ¯”ç»˜å›¾å·¥å…·

å¯ä»¥æ¯”è¾ƒ:
1. ä¸åŒæ¨¡å‹ (PGCN, DGCNN, EEGNet, etc.)
2. åŒä¸€æ¨¡å‹çš„ä¸åŒè¶…å‚æ•°é…ç½®
3. ä¸åŒæ•°æ®é›†åˆ’åˆ†ç­–ç•¥

Usage:
    # è‡ªåŠ¨æ‰«ææ‰€æœ‰æ—¥å¿—
    python utils/plot_comparison_auto.py
    
    # æŒ‡å®šæ—¥å¿—æ–‡ä»¶
    python utils/plot_comparison_auto.py \
        --logs result/PGCN/b32_3sess_frontback.log:PGCN \
               result/DGCNN/b32_lr0.0015.log:DGCNN
"""

import matplotlib.pyplot as plt
import numpy as np
import re
import argparse
from pathlib import Path
from typing import Dict, List


def extract_subject_results(log_file: str, metric: str = 'acc') -> List[float]:
    """
    ä»è®­ç»ƒæ—¥å¿—ä¸­æå–æ¯ä¸ª subject çš„æµ‹è¯•æŒ‡æ ‡
    
    Args:
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        metric: æŒ‡æ ‡åç§° ('acc' æˆ– 'macro-f1')
    
    Returns:
        list: æ¯ä¸ª subject çš„æµ‹è¯•æŒ‡æ ‡åˆ—è¡¨ (ç™¾åˆ†æ¯”å½¢å¼)
    """
    subject_metrics = []
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"âŒ Error reading {log_file}: {e}")
        return []
    
    # æ ¹æ® metric é€‰æ‹©æ­£åˆ™è¡¨è¾¾å¼
    if metric == 'acc':
        pattern = r'best_test_acc:\s*([\d.]+)'
    elif 'macro' in metric or 'f1' in metric:
        pattern = r'best_test_macro-f1:\s*([\d.]+)'
    else:
        pattern = rf'best_test_{metric}:\s*([\d.]+)'
    
    matches = re.findall(pattern, content)
    
    for match in matches:
        value = float(match)
        subject_metrics.append(value * 100)  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
    
    return subject_metrics


def auto_discover_logs(base_dir: str = 'result') -> Dict[str, str]:
    """
    è‡ªåŠ¨å‘ç°è®­ç»ƒæ—¥å¿—æ–‡ä»¶
    
    Args:
        base_dir: ç»“æœç›®å½•
    
    Returns:
        dict: {model_name: log_file_path}
    """
    discovered = {}
    base_path = Path(base_dir)
    
    if not base_path.exists():
        print(f"âŒ Directory not found: {base_dir}")
        return discovered
    
    # æ‰«ææ‰€æœ‰æ¨¡å‹ç›®å½•
    for model_dir in base_path.iterdir():
        if not model_dir.is_dir():
            continue
        
        # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
        log_files = list(model_dir.glob('*.log'))
        
        for log_file in log_files:
            # æ„å»ºå‹å¥½çš„åç§°
            model_name = model_dir.name
            config = log_file.stem  # æ–‡ä»¶å(ä¸å«æ‰©å±•å)
            
            if config.startswith(model_name.lower()):
                # å¦‚æœæ–‡ä»¶ååŒ…å«æ¨¡å‹å,å»æ‰é‡å¤
                label = f"{model_name}"
            else:
                label = f"{model_name}({config})"
            
            discovered[label] = str(log_file)
    
    return discovered


def plot_subject_comparison(
    results_dict: Dict[str, List[float]], 
    save_path: str = 'subject_comparison.png',
    title: str = 'Subject-wise Performance Comparison',
    ylabel: str = 'Accuracy (%)',
    figsize: tuple = (12, 7),
    show_mean: bool = True
):
    """
    ç»˜åˆ¶å¤šæ¨¡å‹åœ¨ä¸åŒ subjects ä¸Šçš„æ€§èƒ½å¯¹æ¯”å›¾
    
    Args:
        results_dict: {model_name: [value1, value2, ...]}
        save_path: ä¿å­˜è·¯å¾„
        title: å›¾è¡¨æ ‡é¢˜
        ylabel: Yè½´æ ‡ç­¾
        figsize: å›¾åƒå¤§å°
        show_mean: æ˜¯å¦æ˜¾ç¤ºå‡å€¼çº¿
    """
    fig, ax = plt.subplots(figsize=figsize)
    
    # é¢„å®šä¹‰é¢œè‰²æ–¹æ¡ˆ(é…è‰²å‚è€ƒè®ºæ–‡å¸¸ç”¨æ–¹æ¡ˆ)
    colors = [
        '#E74C3C',  # Red
        '#F39C12',  # Orange  
        '#3498DB',  # Blue
        '#F1C40F',  # Yellow
        '#2ECC71',  # Green
        '#9B59B6',  # Purple
        '#1ABC9C',  # Turquoise
        '#E67E22',  # Carrot
    ]
    
    # æ ‡è®°æ ·å¼
    markers = ['o', 's', '^', 'D', 'v', 'p', '*', 'h']
    
    # ç»˜åˆ¶æ¯ä¸ªæ¨¡å‹çš„æ›²çº¿
    for idx, (model_name, values) in enumerate(results_dict.items()):
        subjects = list(range(len(values)))
        color = colors[idx % len(colors)]
        marker = markers[idx % len(markers)]
        
        # ç»˜åˆ¶ä¸»æ›²çº¿
        line = ax.plot(subjects, values, 
                       marker=marker, 
                       color=color, 
                       linewidth=2.5, 
                       markersize=7,
                       label=model_name,
                       alpha=0.85,
                       markerfacecolor=color,
                       markeredgecolor='white',
                       markeredgewidth=0.5)
        
        # å¯é€‰:ç»˜åˆ¶å‡å€¼æ°´å¹³çº¿
        if show_mean:
            mean_val = np.mean(values)
            ax.axhline(mean_val, color=color, linestyle='--', 
                      linewidth=1, alpha=0.3)
    
    # è®¾ç½®å›¾è¡¨æ ·å¼
    ax.set_xlabel('Subject', fontsize=14, fontweight='bold')
    ax.set_ylabel(ylabel, fontsize=14, fontweight='bold')
    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)
    
    # å›¾ä¾‹
    ax.legend(loc='best', fontsize=11, framealpha=0.95, 
             edgecolor='gray', fancybox=True, shadow=True)
    
    # ç½‘æ ¼
    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)
    ax.set_axisbelow(True)  # ç½‘æ ¼åœ¨å›¾å½¢ä¸‹æ–¹
    
    # è®¾ç½® Y è½´èŒƒå›´
    all_values = [v for values in results_dict.values() for v in values]
    if all_values:
        y_min = max(0, min(all_values) - 3)
        y_max = min(100, max(all_values) + 2)
        ax.set_ylim(y_min, y_max)
    
    # è®¾ç½® X è½´
    max_subjects = max(len(values) for values in results_dict.values())
    ax.set_xlim(-0.5, max_subjects - 0.5)
    
    # ç¾åŒ–è¾¹æ¡†
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_linewidth(1.5)
    ax.spines['bottom'].set_linewidth(1.5)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
    print(f"\nâœ… Plot saved to: {save_path}")
    plt.close()


def print_statistics(results_dict: Dict[str, List[float]]):
    """
    æ‰“å°è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
    """
    print("\n" + "="*85)
    print("Model Performance Statistics")
    print("="*85)
    print(f"{'Model':<30} {'MeanÂ±Std':<18} {'Min':<10} {'Max':<10} {'Subjects':<8}")
    print("-"*85)
    
    for model_name, values in results_dict.items():
        mean = np.mean(values)
        std = np.std(values)
        min_val = np.min(values)
        max_val = np.max(values)
        n_subjects = len(values)
        
        print(f"{model_name:<30} {mean:>5.2f}Â±{std:<5.2f}%       "
              f"{min_val:>6.2f}%    {max_val:>6.2f}%    {n_subjects:<8}")
    
    print("="*85)


def main():
    parser = argparse.ArgumentParser(description='Plot subject-wise performance comparison')
    parser.add_argument('--logs', nargs='+', 
                       help='Log files in format: path:label (e.g., result/PGCN/run1.log:PGCN-v1)')
    parser.add_argument('--auto', action='store_true',
                       help='Auto-discover all logs in result/ directory')
    parser.add_argument('--result_dir', default='result',
                       help='Base directory for auto-discovery (default: result)')
    parser.add_argument('--metric', default='acc', choices=['acc', 'macro-f1'],
                       help='Which metric to plot (default: acc)')
    parser.add_argument('--output', default='subject_comparison.png',
                       help='Output file path (default: subject_comparison.png)')
    parser.add_argument('--title', default='Subject-wise Performance Comparison',
                       help='Plot title')
    parser.add_argument('--max_models', type=int, default=6,
                       help='Maximum number of models to plot (default: 6)')
    
    args = parser.parse_args()
    
    results = {}
    
    # æ¨¡å¼1: è‡ªåŠ¨å‘ç°
    if args.auto or not args.logs:
        print(f"ğŸ” Auto-discovering logs in {args.result_dir}/...")
        discovered = auto_discover_logs(args.result_dir)
        
        if not discovered:
            print("âŒ No log files found!")
            return
        
        print(f"âœ… Found {len(discovered)} log files:")
        for label, path in list(discovered.items())[:args.max_models]:
            print(f"   - {label}: {path}")
            metrics = extract_subject_results(path, args.metric)
            if metrics:
                results[label] = metrics
    
    # æ¨¡å¼2: æ‰‹åŠ¨æŒ‡å®š
    else:
        print(f"ğŸ“‚ Loading specified logs...")
        for log_spec in args.logs:
            if ':' in log_spec:
                path, label = log_spec.split(':', 1)
            else:
                path = log_spec
                label = Path(path).stem
            
            if not Path(path).exists():
                print(f"âš ï¸  File not found: {path}")
                continue
            
            metrics = extract_subject_results(path, args.metric)
            if metrics:
                results[label] = metrics
                print(f"âœ… Loaded {len(metrics)} subjects from {label}")
    
    if not results:
        print("\nâŒ No valid results extracted!")
        return
    
    # æ‰“å°ç»Ÿè®¡æ•°æ®
    print_statistics(results)
    
    # ç»˜å›¾
    ylabel = 'Accuracy (%)' if args.metric == 'acc' else 'Macro-F1 (%)'
    plot_subject_comparison(
        results,
        save_path=args.output,
        title=args.title,
        ylabel=ylabel
    )
    
    print(f"\nğŸ‰ Done! Check {args.output}")


if __name__ == '__main__':
    main()
